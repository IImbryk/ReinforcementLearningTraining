{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Class_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO2hwXnjwpo5yGAU+BCbrWa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IImbryk/ReinforcementLearningTraining/blob/main/Class_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsmBGFPTwhP6"
      },
      "source": [
        "# Глава 2. Марковские процессы принятия решений  (МППР)  и  динамическое  программирование"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9qxXlakwmaE"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Nuk2LBwy6-F"
      },
      "source": [
        "T = torch.tensor([[0.4, 0.6],\n",
        "                  [0.8, 0.2]]) # определяем матрицу переходов\n",
        "v = torch.tensor([[0.7, 0.3]]) # начальное распределение двух состояний.\n",
        "\n",
        "# В дальнешем будет установлено, что от начального распределения состояний \n",
        "# не зависит распределение вероятностей состояний после k шагов"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc4QTfkSzFVc"
      },
      "source": [
        "T_2 = torch.matrix_power(T, 2) # вероятность перехода после 2 шага\n",
        "T_20 = torch.matrix_power(T, 20) # вероятность перехода после 20 шага"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c02VIMFFzVJd"
      },
      "source": [
        "v_1 = torch.mm(v, T)\n",
        "v_2 = torch.mm(v, T_2)\n",
        "v_20 = torch.mm(v, T_20)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPV_kn1uz87N",
        "outputId": "fc55c400-4ebc-4251-fff8-edf1531ada39"
      },
      "source": [
        "print(\"Вероятность  перехода  после  2  шагов:\\n{}\".format(T_2))\n",
        "print(\"Вероятность  перехода  после  20  шагов:\\n{}\".format(T_20))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Вероятность  перехода  после  2  шагов:\n",
            "tensor([[0.6400, 0.3600],\n",
            "        [0.4800, 0.5200]])\n",
            "Вероятность  перехода  после  20  шагов:\n",
            "tensor([[0.5714, 0.4286],\n",
            "        [0.5714, 0.4286]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohTWKQP10D_-",
        "outputId": "1f45e9c5-32f5-4650-95ad-4eecfd5ff058"
      },
      "source": [
        "print(\"Распределение  состояний  после  1  шага:\\n{}\".format(v_1))\n",
        "print(\"Распределение  состояний  после  2  шага:\\n{}\".format(v_2))\n",
        "print(\"Распределение  состояний  после  20  шага:\\n{}\".format(v_20))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Распределение  состояний  после  1  шага:\n",
            "tensor([[0.5200, 0.4800]])\n",
            "Распределение  состояний  после  2  шага:\n",
            "tensor([[0.5920, 0.4080]])\n",
            "Распределение  состояний  после  20  шага:\n",
            "tensor([[0.5714, 0.4286]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDDNdWNO0ODC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_WS3J2x0sYQ"
      },
      "source": [
        " ## Cоздания МППР"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6y0cLa60uP4"
      },
      "source": [
        "import torch\n",
        "\n",
        "T = torch.tensor(\n",
        "    [[[0.8, 0.1, 0.1],\n",
        "      [0.1, 0.6, 0.3]],\n",
        "     [[0.7, 0.2, 0.1],\n",
        "      [0.1, 0.8, 0.1]],\n",
        "     [[0.6, 0.2, 0.2],\n",
        "      [0.1, 0.4, 0.5]]]\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4HwIrYb1W6E"
      },
      "source": [
        "R = torch.tensor([1., 0., -1.]) # функция вознаграждения\n",
        "gamma = 0.5 # коэффициент дисконтирования\n",
        "\n",
        "action = 0 # оптимальная стратегия - всегда выбирать действие a0"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEBDYJQ-26b9"
      },
      "source": [
        "def cal_value_matrix_inversion(gamma, trans_matrix, reward):\n",
        "  inv = torch.inverse(torch.eye(reward.shape[0]) - gamma * trans_matrix)\n",
        "  V = torch.mm(inv, reward.reshape(-1, 1))\n",
        "  return V"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lWI4yHu3hDd"
      },
      "source": [
        "trans_matrix=T[:,action]\n",
        "V = cal_value_matrix_inversion(gamma, trans_matrix, R) # Функция ценности при оптимальной стратегии"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZcFOuh43oIZ",
        "outputId": "66357c5e-3a1b-4fc6-ef36-31283fe49d47"
      },
      "source": [
        "trans_matrix"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8000, 0.1000, 0.1000],\n",
              "        [0.7000, 0.2000, 0.1000],\n",
              "        [0.6000, 0.2000, 0.2000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i0tAaKO326m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAqeIqhb6_KZ"
      },
      "source": [
        "##Оценивание  стратегии. Приближенное динамическое программирование"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHJSWlE-HSGA"
      },
      "source": [
        "Мы начинаем  со  случайно  выбранных  ценностей  состояний  и  итеративно  обновляем  их,  применяя  уравнение  математического  ожидания    Беллмана,  пока  не достигнем сходимости."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymgJE0u27D__"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP3L9a537BF6"
      },
      "source": [
        "import torch\n",
        "\n",
        "T = torch.tensor(\n",
        "    [[[0.8, 0.1, 0.1],\n",
        "      [0.1, 0.6, 0.3]],\n",
        "     [[0.7, 0.2, 0.1],\n",
        "      [0.1, 0.8, 0.1]],\n",
        "     [[0.6, 0.2, 0.2],\n",
        "      [0.1, 0.4, 0.5]]]\n",
        ")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVifTydv7SSx"
      },
      "source": [
        "R = torch.tensor([1., 0., -1.])\n",
        "gamma = 0.5\n",
        "\n",
        "threshold = 0.0001 # порог остановки процесса оценивания"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF6tR0X37cnN"
      },
      "source": [
        "policy_optimal = torch.tensor([[1.0, 0.0],\n",
        "                               [1.0, 0.0],\n",
        "                               [1.0, 0.0]]) #оптимальная стратегия, при которой всегда выбирается a0"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCCnaBn-79mQ"
      },
      "source": [
        "def policy_evaluation(policy, trans_matrix, rewards, gamma, treshold):\n",
        "  \"\"\"\n",
        "  оценивает стратегию\n",
        "  @param policy: матрица, содержащая вероятности выбора действий в каждом состоянии\n",
        "  \"\"\"\n",
        "  n_state = policy.shape[0]\n",
        "  V = torch.zeros(n_state)\n",
        "  while True:\n",
        "    V_temp = torch.zeros(n_state)\n",
        "    for state, actions in enumerate(policy):\n",
        "      for action, action_prob in enumerate(actions):\n",
        "        V_temp[state] += action_prob * (R[state] + gamma * torch.dot(trans_matrix[state,action], V))\n",
        "    max_delta = torch.max(torch.abs(V - V_temp))\n",
        "    V = V_temp.clone()\n",
        "    if max_delta <= threshold:\n",
        "      break\n",
        "  return V\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTsnXZOzGvrE",
        "outputId": "38395785-6b96-4aa6-aa91-37d7255cff00"
      },
      "source": [
        "V=policy_evaluation(policy_optimal,T,R,gamma,threshold) \n",
        "print( \"Функция  ценности  при  оптимальной  стратегии:\\n{}\".format(V))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Функция  ценности  при  оптимальной  стратегии:\n",
            "tensor([ 1.6786,  0.6260, -0.4821])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIplgf6jG3Fv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oug5p1w7Hw-A"
      },
      "source": [
        "Оценивание  стратегии  используется  для  предсказания результатов стратегии,  а не  для решения задач  управления"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-SEwIgIHyaP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xQ-3bsoIFBU"
      },
      "source": [
        "## Имитация дискретной окружающей среды FrozenLake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNoazMxVIPWy"
      },
      "source": [
        "S: начальное положение; \\\\\n",
        "G: конечное положение, в котором эпизод завершается; \\\\\n",
        "F: замерзшее озеро,  по которому можно ходить; \\\\\n",
        "H: полынья, в которой эпизод завершается"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGR_f2S7Ie8_"
      },
      "source": [
        "Определены  четыре  действия:  влево  (0),  вниз  (1),  вправо  (2)  и  вверх  (3). Агенту  начисляется  вознаграждение  +1,  если  он  успешно  доберется  до  цели, и  0  в  противном  случае.  Пространство  наблюдений  представлено  массивом  из 16 целых  чисел,  а возможных  действий  четыре (естественно). У  этой  среды  есть  особенность,  осложняющая  обучение:  поскольку  лед скользкий, агент не  всегда  движется туда, куда  собирался. Например, он  может сдвинуться влево или вправо,  хотя намеревался идти вниз."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmSqsTfsIMzV",
        "outputId": "1243bf38-61d7-4b5a-ef6f-8ba9c065fbdb"
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "env = gym.make('FrozenLake-v0')\n",
        "n_state = env.observation_space.n\n",
        "n_action = env.action_space.n\n",
        "\n",
        "print(n_state, n_action)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k5PKMo8MElV",
        "outputId": "179a0bf0-f64b-4278-c3f0-b61c50cedbc8"
      },
      "source": [
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26qIMPjPMjsD"
      },
      "source": [
        "new_state, reward, is_done, info = env.step(1)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgkJtZlnMt1U",
        "outputId": "90383226-3031-432b-9481-44b6b04c3e25"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_TGkUFpMwAh"
      },
      "source": [
        "def run_episode(env, policy):\n",
        "  state = env.reset()\n",
        "  total_reward = 0\n",
        "  is_done = False\n",
        "  while not is_done:\n",
        "    action = policy[state].item()\n",
        "    state, reward, is_done, infp = env.step(action)\n",
        "    total_reward += reward\n",
        "    if is_done:\n",
        "      break\n",
        "\n",
        "  return total_reward"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnqPi1mpN8hr"
      },
      "source": [
        "n_episode = 1000\n",
        "total_rewards = []\n",
        "for episode in range(n_episode):\n",
        "  random_policy = torch.randint(high=n_action, size=(n_state,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QEUvnRNd0f5"
      },
      "source": [
        "##Решение МППР С Помощью алгоритма итерации по ценности"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VKKdause0n0"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjtJ6cvRe5OT"
      },
      "source": [
        "Вычислив оптимальную ценность, получаем оптимальную стратегию:\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIzv_bNZe-qB"
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "env = gym.make('FrozenLake-v0')\n",
        "\n",
        "gamma = 0.99 # при малом  коэффициенте  предпочтение отдается  непосредственному  вознаграждению,  а  при  большом  учитываются и будущие вознаграждения.\n",
        "threshold = 0.0001"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PCjsp9tfRq1"
      },
      "source": [
        "def value_iteration(env, gamma, threashold):\n",
        "  n_state = env.observation_space.n\n",
        "  n_action = env.action_space.n\n",
        "\n",
        "  V = torch.zeros(n_state)\n",
        "\n",
        "  while True:\n",
        "    V_temp = torch.empty(n_state)\n",
        "    for state in range(n_state):\n",
        "      v_actions = torch.zeros(n_action)\n",
        "      for action in range(n_action):\n",
        "        for trans_prob, new_state, reward, _ in env.env.P[state][action]:\n",
        "          v_actions[action] += trans_prob * (reward + gamma * V[new_state])\n",
        "      V_temp[state] = torch.max(v_actions)\n",
        "    max_delta = torch.max(torch.abs(V - V_temp))\n",
        "    V = V_temp.clone()\n",
        "    if max_delta <= threshold:\n",
        "        break\n",
        "  return V\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSDfchUthIBA"
      },
      "source": [
        "def extract_optimal_policy(env, V_optimal, gamma):\n",
        "  n_state = env.observation_space.n\n",
        "  n_action = env.action_space.n\n",
        "\n",
        "  optimal_policy = torch.zeros(n_state)\n",
        "\n",
        "  for state in range(n_state):\n",
        "    v_action = torch.zeros(n_action)\n",
        "    for action in range(n_action):\n",
        "      for trans_prob, new_state, reward, _ in env.env.P[state][action]:\n",
        "        v_action[action] += trans_prob * (reward + gamma * V_optimal[new_state])\n",
        "    optimal_policy[state] = torch.argmax(v_action)\n",
        "  return optimal_policy"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkqDSP3Vi6RG",
        "outputId": "ea4f367a-cc04-4000-fa64-c7269c75ce56"
      },
      "source": [
        "V_optimal=value_iteration(env,gamma,threshold)\n",
        "print('Оптимальные  ценности:\\n{}'.format(V_optimal))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Оптимальные  ценности:\n",
            "tensor([0.5404, 0.4966, 0.4681, 0.4541, 0.5569, 0.0000, 0.3572, 0.0000, 0.5905,\n",
            "        0.6421, 0.6144, 0.0000, 0.0000, 0.7410, 0.8625, 0.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjfJ6xKZjHB2",
        "outputId": "9ede4135-75f1-4326-b897-3c78c5a12ae6"
      },
      "source": [
        "optimal_policy=extract_optimal_policy(env,V_optimal,gamma)\n",
        "print('Оптимальная стратегия:\\n{}'.format(optimal_policy))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Оптимальная стратегия:\n",
            "tensor([0., 3., 3., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKcQr7jmjkEX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asX6wq2ew6br"
      },
      "source": [
        "##Игра с подбрасыванием монеты"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyb2O9OYxDI1"
      },
      "source": [
        "В  каждом  раунде игрок  ставит  на  выпадение  орла.  Если  действительно  выпал  орел,  игрок  получает ту  сумму, которую  поставил, в  противном  случае теряет  свою  ставку. Игра продолжается  до  разорения  игрока  или  до  выигрыша  определенной  суммы (скажем,  больше  100  долларов).  Предположим,  что  монета  несимметричная, так  что  орел  выпадает  в  40  %  случаев.  Сколько  должен  поставить  игрок,  чтобы  максимизировать  шансы  на  выигрыш,  с  учетом  своего  текущего  капитала в каждом раунде? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5uRgNutxRF7"
      },
      "source": [
        "Состояние - это капитал игрока в долларах (всего 101 состояние) \\\\\n",
        "Вознаграждение -1 если достигнуто 100+, иначе 0 \\\\\n",
        "Действие - это сумма, которую игрок ставит в раунде(1,2... 100-s) \\\\\n",
        "Терминальные состояние - это 0 или 100+ \\\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QAen3t_w9RJ"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6xblRt9yEhR"
      },
      "source": [
        "gamma = 1 #здесь обесценивания нет\n",
        "threshold = 1e-10 # порог сходимости\n",
        "\n",
        "\n",
        "capital_max = 100\n",
        "n_state = capital_max + 1\n",
        "head_prob = 0.4\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThOOBnbLyZJS"
      },
      "source": [
        "rewards = torch.zeros(n_state)\n",
        "rewards[-1] = 1"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGJqPBPsyv5i"
      },
      "source": [
        "env = {'capital_max':capital_max,\n",
        "       'head_prob': head_prob,\n",
        "       'rewards': rewards,\n",
        "       'n_state': n_state}"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8x9_BHYzgAh"
      },
      "source": [
        "\n",
        "def value_iteration(env, gamma, threshold):\n",
        "  \"\"\"\n",
        "  алгоритм итерации по ценности\n",
        "  \"\"\"\n",
        "\n",
        "  head_prob=env['head_prob']\n",
        "  n_state=env['n_state']\n",
        "  capital_max=env['capital_max']\n",
        "\n",
        "  V = torch.zeros(n_state)\n",
        "  while True:\n",
        "    V_temp = torch.zeros(n_state)\n",
        "    for state in range(1, capital_max):\n",
        "      v_actions = torch.zeros(min(state, capital_max - state) + 1)\n",
        "      for action in range(1, min(state, capital_max - state) + 1):\n",
        "        v_actions[action] += head_prob * (rewards[state+action] + gamma*V[state + action])\n",
        "        v_actions[action] += (1-head_prob) * (rewards[state-action] + gamma*V[state - action])\n",
        "      V_temp[state] = torch.max(v_actions)\n",
        "    max_delta = torch.max(torch.abs(V - V_temp))\n",
        "    V = V_temp.clone()\n",
        "    if max_delta <= threshold:\n",
        "      break\n",
        "  return V\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-_UvV1iy95c"
      },
      "source": [
        "def extract_optimal_policy(env, V_optimal, gamma):\n",
        "  \"\"\"\n",
        "  построение оптимальной стратегии по оптимальным ценностям\n",
        "  \"\"\"\n",
        "\n",
        "  head_prob=env['head_prob']\n",
        "  n_state=env['n_state']\n",
        "  capital_max=env['capital_max']\n",
        "\n",
        "  optimal_policy = torch.zeros(capital_max).int()\n",
        "\n",
        "  for state in range(1, capital_max):\n",
        "    v_actions = torch.zeros(capital_max)\n",
        "    for action in range(1, min(state, capital_max - state) + 1):\n",
        "      v_actions[action] += head_prob * (rewards[state+action] + gamma*V_optimal[state + action])\n",
        "      v_actions[action] += (1-head_prob) * (rewards[state-action] + gamma*V_optimal[state - action])\n",
        "    optimal_policy[state] = torch.argmax(v_actions)\n",
        "  return optimal_policy"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fT17GU6m2eLv",
        "outputId": "085d190f-8432-4cc2-8510-bb46edce0e9a"
      },
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "V_optimal = value_iteration(env, gamma, threshold)\n",
        "optimal_policy = extract_optimal_policy(env, V_optimal, gamma)\n",
        "print(\"Для решения методом итерации по ценности понадобилось {:.3f}с\".format(time.time()-start_time))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Для решения методом итерации по ценности понадобилось 6.010с\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYK3MTsW3Iqa",
        "outputId": "7d72bc9e-f2b8-4e34-8095-6acc59e9c23b"
      },
      "source": [
        "print('Оптимальные  ценности:\\n{}'.format(V_optimal)) \n",
        "print('Оптимальная  стратегия:\\n{}'.format(optimal_policy))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Оптимальные  ценности:\n",
            "tensor([0.0000, 0.0021, 0.0052, 0.0092, 0.0129, 0.0174, 0.0231, 0.0278, 0.0323,\n",
            "        0.0377, 0.0435, 0.0504, 0.0577, 0.0652, 0.0695, 0.0744, 0.0807, 0.0866,\n",
            "        0.0942, 0.1031, 0.1087, 0.1160, 0.1259, 0.1336, 0.1441, 0.1600, 0.1631,\n",
            "        0.1677, 0.1738, 0.1794, 0.1861, 0.1946, 0.2017, 0.2084, 0.2165, 0.2252,\n",
            "        0.2355, 0.2465, 0.2579, 0.2643, 0.2716, 0.2810, 0.2899, 0.3013, 0.3147,\n",
            "        0.3230, 0.3339, 0.3488, 0.3604, 0.3762, 0.4000, 0.4031, 0.4077, 0.4138,\n",
            "        0.4194, 0.4261, 0.4346, 0.4417, 0.4484, 0.4565, 0.4652, 0.4755, 0.4865,\n",
            "        0.4979, 0.5043, 0.5116, 0.5210, 0.5299, 0.5413, 0.5547, 0.5630, 0.5740,\n",
            "        0.5888, 0.6004, 0.6162, 0.6400, 0.6446, 0.6516, 0.6608, 0.6690, 0.6791,\n",
            "        0.6919, 0.7026, 0.7126, 0.7248, 0.7378, 0.7533, 0.7697, 0.7868, 0.7965,\n",
            "        0.8075, 0.8215, 0.8349, 0.8520, 0.8721, 0.8845, 0.9009, 0.9232, 0.9406,\n",
            "        0.9643, 0.0000])\n",
            "Оптимальная  стратегия:\n",
            "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,  9, 17,\n",
            "        18,  6,  5, 21,  3,  2,  1, 25,  1,  2,  3, 29,  5,  6,  7,  8,  9, 35,\n",
            "        36, 12, 12, 11, 10,  9,  8,  7, 44,  5,  4,  3,  2,  1, 50,  1,  2,  3,\n",
            "         4,  5,  6,  7,  8,  9, 10, 11, 12, 12, 11, 10,  9,  8,  7,  6,  5,  4,\n",
            "         3,  2,  1, 25,  1,  2,  3, 21,  5, 19,  7,  8, 16, 15, 14, 12, 12, 11,\n",
            "        10,  9,  8,  7,  6,  5,  4,  3,  2,  1], dtype=torch.int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HoaGuoG33FE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}